# https://github.com/maidacundo/real-time-fire-segmentation-deep-learning/blob/main/Fire%20Segmentation%20Pipeline.ipynb
import threading
import random
import rasterio
import os
import numpy as np
import sys
import pandas as pd
from sklearn.utils import shuffle as shuffle_lists
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms
from torchvision import models
from torch.utils.data import DataLoader, Dataset
from torch.autograd import Variable
import copy
import joblib

MAX_PIXEL_VALUE = 65535 # 이미지 정규화를 위한 픽셀 최대값

class threadsafe_iter:
    """
    데이터 불러올떼, 호출 직렬화
    """
    def __init__(self, it):
        self.it = it
        self.lock = threading.Lock()

    def __iter__(self):
        return self

    def __next__(self):
        with self.lock:
            return self.it.__next__()


def threadsafe_generator(f):
    def g(*a, **kw):
        return threadsafe_iter(f(*a, **kw))

    return g

def get_img_arr(path):
    img = rasterio.open(path).read().transpose((1, 2, 0))    
    img = np.float32(img)/MAX_PIXEL_VALUE
    
    return img

def get_img_762bands(path):
    img = rasterio.open(path).read((7,6,2)).transpose((1, 2, 0))    
    img = np.float32(img)/MAX_PIXEL_VALUE
    
    return img

def get_img_762bands_test(path):
    img = rasterio.open(path).read((7,6,2))  # 채널, 높이, 너비 순서
    img = np.float32(img) / MAX_PIXEL_VALUE
    # img = img.transpose((1, 2, 0))  # 이전 방식 (높이, 너비, 채널)
    # PyTorch가 기대하는 형식으로 이미지 차원 변경 불필요
    return img

    
def get_mask_arr(path):
    img = rasterio.open(path).read().transpose((1, 2, 0))
    seg = np.float32(img)
    return seg

@threadsafe_generator
def generator_from_lists(images_path, masks_path, batch_size=32, shuffle = True, random_state=None, image_mode='10bands'):
   
    images = []
    masks = []

    fopen_image = get_img_arr
    fopen_mask = get_mask_arr

    if image_mode == '762':
        fopen_image = get_img_762bands

    i = 0 
    # 데이터 shuffle
    while True:
        
        if shuffle:
            if random_state is None:
                images_path, masks_path = shuffle_lists(images_path, masks_path)
            else:
                images_path, masks_path = shuffle_lists(images_path, masks_path, random_state= random_state + i)
                i += 1 


        for img_path, mask_path in zip(images_path, masks_path):
            
            img = fopen_image(img_path)
            mask = fopen_mask(mask_path)
            images.append(img)
            masks.append(mask)

            if len(images) >= batch_size:
                yield (np.array(images), np.array(masks))
                images = []
                masks = []




def dice_coef(y_true, y_pred, smooth=1):
    intersection = torch.sum(y_true * y_pred, dim=[1,2,3])
    union = torch.sum(y_true, dim=[1,2,3]) + torch.sum(y_pred, dim=[1,2,3])
    dice = torch.mean((2. * intersection + smooth) / (union + smooth))
    return dice

def pixel_accuracy(y_true, y_pred):
    y_pred = torch.sigmoid(y_pred) > 0.5  # Sigmoid activation and thresholding
    correct = torch.eq(y_pred, y_true).sum().float()
    total = torch.numel(y_true)
    return correct / total

# Custom Dataset 클래스
class CustomDataset(Dataset):
    def __init__(self, image_paths, mask_paths, transform=None):
        self.image_paths = image_paths
        self.mask_paths = mask_paths
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        mask_path = self.mask_paths[idx]
        img = get_img_762bands(img_path)
        mask = get_mask_arr(mask_path)
        
        if self.transform:
            img = self.transform(img)
            mask = self.transform(mask)
            
        return img, mask


# 사용할 데이터의 meta정보 가져오기

train_meta = pd.read_csv('./dataset/train_meta.csv')
test_meta = pd.read_csv('./dataset/test_meta.csv')


# 저장 이름
save_name = 'base_line'

N_FILTERS = 16 # 필터수 지정
N_CHANNELS = 3 # channel 지정
EPOCHS = 30 # 훈련 epoch 지정
BATCH_SIZE = 64 # batch size 지정
IMAGE_SIZE = (256, 256) # 이미지 크기 지정
# IMAGE_SIZE = (512, 512) # 이미지 크기 지정
MODEL_NAME = 'FireSegmentationModel' # 모델 이름
RANDOM_STATE = 42 # seed 고정
INITIAL_EPOCH = 0 # 초기 epoch

# 데이터 위치
IMAGES_PATH = './dataset/train_img/'
MASKS_PATH = './dataset/train_mask/'

# 가중치 저장 위치
OUTPUT_DIR = './train_output/'
WORKERS = 4

# 조기종료
EARLY_STOP_PATIENCE = 5 

# 중간 가중치 저장 이름
CHECKPOINT_PERIOD = 5
CHECKPOINT_MODEL_NAME = 'checkpoint-{}-{}-epoch_{{epoch:02d}}.hdf5'.format(MODEL_NAME, save_name)
 
# 최종 가중치 저장 이름
FINAL_WEIGHTS_OUTPUT = 'model_{}_{}_final_weights.h5'.format(MODEL_NAME, save_name)

# 사용할 GPU 이름
CUDA_DEVICE = 0


# 저장 폴더 없으면 생성
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)


# PyTorch에서 GPU 사용 가능 여부 확인
if torch.cuda.is_available():
    device = torch.device(f'cuda:{CUDA_DEVICE}') # GPU 디바이스 설정
    torch.cuda.set_device(device) # PyTorch에 사용할 기본 디바이스로 설정
    print(f'Using GPU: {torch.cuda.get_device_name(device)}')
else:
    device = torch.device('cpu')
    print('GPU is not available. Using CPU instead.')

# train : val = 8 : 2 나누기
x_tr, x_val = train_test_split(train_meta, test_size=0.2, random_state=RANDOM_STATE)
print(len(x_tr), len(x_val))

# 데이터 경로 설정
images_train = [os.path.join('./dataset/train_img/', image) for image in x_tr['train_img']]
masks_train = [os.path.join('./dataset/train_mask/', mask) for mask in x_tr['train_mask']]
images_validation = [os.path.join('./dataset/train_img/', image) for image in x_val['train_img']]
masks_validation = [os.path.join('./dataset/train_mask/', mask) for mask in x_val['train_mask']]

# 데이터 변환 정의
transform = transforms.Compose([transforms.ToTensor()])

# DataLoader 생성
train_loader = DataLoader(CustomDataset(images_train, masks_train, transform), batch_size=64, shuffle=True)
val_loader = DataLoader(CustomDataset(images_validation, masks_validation, transform), batch_size=64, shuffle=False)



from typing import Tuple
import torch
from torch import nn

"""Module providing classes to define the modules of the network."""
from typing import Tuple
import torch
from torch import nn
import torch.nn.functional as F

class MobileBottleNeck(nn.Module):
    """
    A class for the MobileNetV3 bottleneck block.

    Attributes
    ----------
    use_skip_conn: bool
        Whether to use the skip connection between the input and the
        point-wise convolution results or not.
    standard_conv: Sequential
        Standard convolution sequential module.
    depthwise_conv: Sequential
        Depth-wise convolution sequential module.
    squeeze_excitation: Sequential
        Squeeze and excitation sequential module if demanded. None otherwise.
    pointwise_conv: Sequential
        Point-wise convolution sequential module.
    
    Methods
    -------
    forward(x: FloatTensor) -> FloatTensor
        Forward pass of the MobileNetV3 bottleneck block.
    """
    def __init__(
        self, in_channels: int, expansion_channels: int, out_channels: int,
        depthwise_kernel_size: int, activation_layer: nn.Module,
        use_squeeze_excitation: bool, stride: int = 1,
        padding: int = 1) -> None:
        """Initialize the MobileNetV3 bottleneck block.

        Parameters
        ----------
        in_channels: int
            Number of input channels.
        expansion_channels: int
            Number of channels of the hidden layers.
        out_channels: int
            Number of output channels.
        depthwise_kernel_size: int
            Size of the depth-wise convolutional kernel.
        activation_layer: Module
            Activation function to use after convolutional layers.
        use_squeeze_excitation: bool
            Whether to use the squeeze and excitation block or not.
        stride: int
            Stride size for convolutional layers, by default 1.
        padding: int
            Padding size for convolutional layers, by default 1.

        """
        super().__init__()

        # Set whether to use skip connection or not.
        self.use_skip_conn = stride == 1 and in_channels == out_channels
        # Set whether to use the squeeze and excitation module or not.
        self.use_squeeze_excitation = use_squeeze_excitation

        # Set standard convolution sequential module.
        self.standard_conv = nn.Sequential(
            nn.Conv2d(in_channels, expansion_channels, kernel_size=1,
                      bias=False),
            nn.BatchNorm2d(expansion_channels, track_running_stats=False),
            activation_layer(),
        )

        # Set depth-wise convolution sequential module.
        self.depthwise_conv = nn.Sequential(
            nn.Conv2d(expansion_channels, expansion_channels,
                      kernel_size=depthwise_kernel_size, stride=stride,
                      groups=expansion_channels, padding=padding, bias=False),
            nn.BatchNorm2d(expansion_channels, track_running_stats=False),
        )

        # Set squeeze and excitation sequential module.
        self.squeeze_excitation = nn.Sequential(
            nn.AdaptiveAvgPool2d(output_size=1),
            nn.Conv2d(expansion_channels, in_channels, kernel_size=1),
            nn.ReLU(),
            nn.Conv2d(in_channels, expansion_channels, kernel_size=1),
            nn.Hardswish(),
        ) if use_squeeze_excitation else None

        # Set point-wise convolution sequential module.
        self.pointwise_conv = nn.Sequential( 
            nn.Conv2d(expansion_channels, out_channels, kernel_size=1,
                      bias=False),
            nn.BatchNorm2d(out_channels, track_running_stats=False)
        )

    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:
        """Forward pass of the MobileNetV3 bottleneck block.

        Parameters
        ----------
        x : FloatTensor
            Input tensor.

        Returns
        -------
        FloatTensor
            Output tensor.
        """
        # Apply standard convolution.
        out = self.standard_conv(x)

        # Apply depth-wise convolution.
        depth_wise_out = self.depthwise_conv(out)

        # Apply squeeze and excitation block if demanded.
        if self.use_squeeze_excitation:
            out = self.squeeze_excitation(depth_wise_out)
            out = out * depth_wise_out
        else:
            out = depth_wise_out

        # Apply point-wise convolution
        out = self.pointwise_conv(out)

        # Apply an additive skip connection with the input if demanded.
        if self.use_skip_conn:
            out = out + x

        return out

class DCNN(nn.Module):
    """A Deep Convolutional Neural Network (DCNN) module based on MobileNetV3
    architecture.

    This class implements a DCNN module with four sequential blocks, where the
    first block is the input convolutional layer, and the next three blocks
    are sequences of MobileNetV3 bottlenecks.

    Attributes
    ----------
        input_convolution : Sequential
            A sequential block consisting of a 2D convolutional layer,
            followed by batch normalization and Hardswish activation.
        bottlenecks_sequential_1 : Sequential 
            A sequential block consisting of a MobileNetV3 bottleneck
            layer.
        bottlenecks_sequential_2 : Sequential
            A sequential block consisting of two MobileNetV3 bottleneck
            layers.
        bottlenecks_sequential_3 : Sequential
            A sequential block consisting of three MobileNetV3 bottleneck
            layers.
        bottlenecks_sequential_4 : Sequential
            A sequential block consisting of six MobileNetV3 bottleneck
            layers.

    Methods
    -------
    forward(x: FloatTensor) -> (
    FloatTensor, FloatTensor, FloatTensor, FloatTensor)
        Forward pass of the DCNN module. Takes a tensor as input and 
        returns four tensors: `f1`, `f2`, `f3`, and `out`, which
        represent the intermediate feature maps after each of the
        three first bottleneck blocks and the final output of the module,
        respectively.
    """
    def __init__(self) -> None:
        """Initialize the DCNN module."""
        super().__init__()

        # Input sequential block.
        self.input_convolution = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(16, track_running_stats=False),
            nn.Hardswish(),
        )

        # First MobileNetV3 bottlenecks sequential block.
        self.bottlenecks_sequential_1 =  nn.Sequential(
            MobileBottleNeck(16, 16, 16, 3, nn.ReLU, False, stride=1),
        )

        # Second MobileNetV3 bottlenecks sequential block.
        self.bottlenecks_sequential_2 =  nn.Sequential(
            MobileBottleNeck(16, 64, 24, 3, nn.ReLU, False, stride=2,
                             padding=1),
            MobileBottleNeck(24, 72, 24, 3, nn.ReLU, False, stride=1),
        )

        # Third MobileNetV3 bottlenecks sequential block.
        self.bottlenecks_sequential_3 =  nn.Sequential(
            MobileBottleNeck(24, 72, 40, 5, nn.ReLU, True, stride=2,
                             padding=2),
            MobileBottleNeck(40, 120, 40, 5, nn.ReLU, True, stride=1,
                             padding=2),
            MobileBottleNeck(40, 120, 40, 5, nn.ReLU, True, stride=1,
                             padding=2),
        )

        # Last MobileNetV3 bottlenecks sequential block.
        self.bottlenecks_sequential_4 =  nn.Sequential(
            MobileBottleNeck(40, 240, 80, 3, nn.Hardswish, False, stride=2),
            MobileBottleNeck(80, 200, 80, 3, nn.Hardswish, False, stride=1),
            MobileBottleNeck(80, 184, 80, 3, nn.Hardswish, False, stride=1),
            MobileBottleNeck(80, 184, 80, 3, nn.Hardswish, False, stride=1),
            MobileBottleNeck(80, 480, 112, 3, nn.Hardswish, True, stride=1),
            MobileBottleNeck(112, 672, 160, 3, nn.Hardswish, True, stride=1),
            MobileBottleNeck(160, 672, 160, 5, nn.Hardswish, True, stride=1,
                             padding=2),
            MobileBottleNeck(160, 960, 160, 5, nn.Hardswish, True, stride=1,
                             padding=2),
            MobileBottleNeck(160, 960, 160, 5, nn.Hardswish, True, stride=1,
                             padding=2),
        )

    def forward(self, x: torch.FloatTensor) -> Tuple[
        torch.FloatTensor, torch.FloatTensor,
        torch.FloatTensor, torch.FloatTensor]:
        """Forward pass of the DCNN block.

        Parameters
        ----------
        x : FloatTensor
            The input tensor.

        Returns
        -------
        FloatTensor
            The first shallow intermediate feature `f1`.
        FloatTensor
            The second shallow intermediate feature `f2`.
        FloatTensor
            The third shallow intermediate feature `f3`.
        FloatTensor
            The output tensor.
        """
        # Apply the initial convolution.
        out = self.input_convolution(x)
        # Apply the series of bottleneck blocks and get the intermediate
        # feature results.
        f1 = self.bottlenecks_sequential_1(out)
        f2 = self.bottlenecks_sequential_2(f1)
        f3 = self.bottlenecks_sequential_3(f2)
        out = self.bottlenecks_sequential_4(f3)

        return f1, f2, f3, out

class ASPP(nn.Module):
    """An Atrous Spatial Pyramid Pooling (ASPP) module based on DeepLabV3+
    architecture.

    This class applies a series of atrous convolutions on an input tensor
    followed by global average pooling to extract multi-scale contextual
    information.

    Attributes
    ----------
    standard_convolution : Sequential
        A sequential block consisting of a convolution layer, batch
        normalization layer, and ReLU activation function.
    atrous_convolution_1 : Sequential
        A sequential block performing an atrous convolution with a dilation
        rate of 6.
    atrous_convolution_2 : Sequential
        A sequential block performing an atrous convolution with a dilation
        rate of 12.
    atrous_convolution_3 : Sequential
        A sequential block performing an atrous convolution with a dilation
        rate of 18.
    global_average_pooling : Sequential 
        A sequential block performing global average pooling.
    final_convolution : Sequential
        A final sequential block consisting of a convolution layer followed
        by ReLU activation and dropout.

    Methods
    -------
    forward(x: FloatTensor) -> FloatTensor
        Computes the forward pass of the ASPP module. Takes an input
        tensor and applies the series of atrous convolutions and global
        average pooling, concatenates the resulting tensors, and applies
        a final convolution.
    """
    def __init__(self) -> None:
        """Initialize the ASPP module."""
        super().__init__()
        # Set the number of input and output channels.
        in_channels = 160
        out_channels = 256

        # Set the standard convolution sequential block.
        self.standard_convolution = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),
            nn.BatchNorm2d(out_channels, track_running_stats=False),
            nn.ReLU(),
        )

        # Set the atrous convolution sequential blocks.
        self.atrous_convolution_1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1,
                      bias=False, dilation=6, padding=6),
            nn.BatchNorm2d(out_channels, track_running_stats=False),
            nn.ReLU(),
        )
        self.atrous_convolution_2 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1,
                      bias=False, dilation=12, padding=12),
            nn.BatchNorm2d(out_channels, track_running_stats=False),
            nn.ReLU(),
        )
        self.atrous_convolution_3 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1,
                      bias=False, dilation=18, padding=18),
            nn.BatchNorm2d(out_channels, track_running_stats=False),
            nn.ReLU(),
        )

        # Set the global average pooling sequential block.
        self.global_average_pooling = nn.Sequential(
            nn.AdaptiveAvgPool2d(output_size=1),
            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1,
                      bias=False),
            nn.BatchNorm2d(out_channels, track_running_stats=False),
            nn.ReLU(),
        )

        # Set the output convolution sequential block.
        self.final_convolution = nn.Sequential(
            nn.Conv2d(out_channels*5, out_channels, kernel_size=1, stride=1,
                      bias=False),
            nn.BatchNorm2d(out_channels, track_running_stats=False),
            nn.ReLU(),
            nn.Dropout(0.1),
        )

    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:
        """Forward pass of the ASPP block.

        Parameters
        ----------
        x : FloatTensor
            The input tensor.

        Returns
        -------
        FloatTensor
            The output tensor.
        """
        # Apply the series of atrous convolutions on the input.
        out1 = self.standard_convolution(x)
        out2 = self.atrous_convolution_1(x)
        out3 = self.atrous_convolution_2(x)
        out4 = self.atrous_convolution_3(x)

        # Apply Global Average Pooling on the input and replicate
        # spatially the result.
        out5 = self.global_average_pooling(x)
        out5 = F.interpolate(out5, size=x.shape[-2:], mode='bilinear',
                             align_corners=False)

        # Concatenate the tensors of the atrous convolutions and the pooling
        # operation and apply a final convolution.
        out = torch.cat([out1, out2, out3, out4, out5], dim=1)
        f4 = self.final_convolution(out)

        return f4

class Encoder(nn.Module):
    """The encoder module of the network.
    It is composed of a Deep Convolutional Neural Network (DCNN) module and
    an Atrous Spatial Pyramid Pooling (ASPP) module.

    Attributes
    ----------
    dcnn : DCNN
        The DCNN module of the encoder.
    aspp : ASPP
        The ASPP module of the encoder.
        
    Methods
    -------
    forward(x: FloatTensor) -> (
    FloatTensor, FloatTensor, FloatTensor, FloatTensor)
        Forward pass of the encoder module. Takes a tensor as input and 
        returns four tensors: `f1`, `f2`, `f3`, and `f4`, which
        represent the intermediate feature maps after each of the
        three first bottleneck blocks and the final output of the encoder,
        respectively.
    """
    def __init__(self) -> None:
        """Initialize the encoder module."""
        super().__init__()
        # Set the DCNN module of the encoder.
        self.dcnn = DCNN()
        # Set the ASPP module of the encoder.
        self.aspp = ASPP()

    def forward(self, x: torch.FloatTensor) -> Tuple[
        torch.FloatTensor, torch.FloatTensor, 
        torch.FloatTensor, torch.FloatTensor]:
        """Forward pass of the encoder block.

        Parameters
        ----------
        x : FloatTensor
            The input tensor.

        Returns
        -------
        FloatTensor
            The first shallow intermediate feature `f1`.
        FloatTensor
            The second shallow intermediate feature `f2`.
        FloatTensor
            The third shallow intermediate feature `f3`.
        FloatTensor
            The output of the encoder's ASPP module: `f4`.
        """
        # Get the first 3 shallow intermediate features and the
        # DCNN module output.
        f1, f2, f3, out = self.dcnn(x)
        # Get the ASPP module output.
        f4 = self.aspp(out)
        return f1, f2, f3, f4

class Decoder(nn.Module):
    """The decoder module of the network.
    It is composed of a Deep Convolutional Neural Network (DCNN) module and
    an Atrous Spatial Pyramid Pooling (ASPP) module.

    Attributes
    ----------
    convolution_f1 : Sequential
        The sequential block of convolutional layers for the shallow
        intermediate feature `f1`.
    convolution_f2 : Sequential
        The sequential block of convolutional layers for the shallow
        intermediate feature `f2`.
    convolution_f3 : Sequential
        The sequential block of convolutional layers for the shallow
        intermediate feature `f3`.
    upsample_f1 : Upsample
        The upsampling layer for the shallow intermediate feature `f1`.
    upsample_f3 : Upsample
        The upsampling layer for the shallow intermediate feature `f3`.
    upsample_f4 : Upsample
        The upsampling layer for the shallow intermediate feature `f4`.
    final_convolution : Sequential
        The final sequential block of convolutional layers for the
        channel-wise concatenated intermediate features.
    final_upsample : Upsample
        The final upsampling layer for the concatenated intermediate
        features.
        
    Methods
    -------
    forward(f1: FloatTensor, f2: FloatTensor, f3: FloatTensor,
    f4: FloatTensor) ->  FloatTensor
        Forward pass of the decoder module.
    """
    def __init__(self, target_size: Tuple[int, int]) -> None:
        """Initialize the decoder module.

        Parameters
        ----------
        target_size : (int, int)
            The size of the output tensor.
        """
        super().__init__()
        # Set the output channels of each convolution applied to the
        # shallow intermediate features.
        out_channels = 256
        # Set the size of the shallow feature `f2`, which is the target
        # size of the upsampling of the other shallow features
        f2_size = (128, 128)

        # Set the convolution sequential blocks to assign the desired equal
        # output channels to each shallow feature.
        self.convolution_f1 = nn.Sequential(
            nn.Conv2d(16, out_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_channels, track_running_stats=False),
            nn.ReLU(),
        )
        self.convolution_f2 = nn.Sequential(
            nn.Conv2d(24, out_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_channels, track_running_stats=False),
            nn.ReLU(),
        )
        self.convolution_f3 = nn.Sequential(
            nn.Conv2d(40, out_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_channels, track_running_stats=False),
            nn.ReLU(),
        )

        # Set the upsampling operation layers to change the shape of
        # each shallow feature to the size of `f2`.
        self.upsample_f1 = nn.Upsample(f2_size, mode='bilinear',
                                       align_corners=False)
        self.upsample_f3 = nn.Upsample(f2_size, mode='bilinear',
                                       align_corners=False)
        self.upsample_f4 = nn.Upsample(f2_size, mode='bilinear',
                                       align_corners=False)

        # Set the final convolution sequential block.
        self.final_convolution = nn.Sequential(
            nn.Conv2d(out_channels * 4, 256, kernel_size=3, bias=False),
            nn.BatchNorm2d(256, track_running_stats=False),
            nn.ReLU(),
            # 마지막 출력 채널을 1로 설정하여 타겟과 일치시킴
            nn.Conv2d(256, 1, kernel_size=1, stride=1),
        )

        # Set the final upsample block to change the image to the target size.
        self.final_upsample = nn.Upsample(target_size, mode='bilinear', align_corners=False)

    def forward(
        self, f1: torch.FloatTensor, f2: torch.FloatTensor,
        f3: torch.FloatTensor, f4: torch.FloatTensor) -> torch.FloatTensor:
        
        # Convolution을 통해 채널 크기 표준화
        out_f1 = self.convolution_f1(f1)
        out_f2 = self.convolution_f2(f2)
        out_f3 = self.convolution_f3(f3)
        # f4는 이미 ASPP를 통해 처리되었으므로 여기서 다시 처리할 필요 없음
        out_f4 = f4

        # 모든 텐서를 동일한 target 크기로 업샘플링
        target_size = (128, 128)  # 예시 목표 크기, 모델에 따라 조정
        out_f1 = nn.functional.interpolate(out_f1, size=target_size, mode='bilinear', align_corners=False)
        out_f2 = nn.functional.interpolate(out_f2, size=target_size, mode='bilinear', align_corners=False)
        out_f3 = nn.functional.interpolate(out_f3, size=target_size, mode='bilinear', align_corners=False)
        out_f4 = nn.functional.interpolate(out_f4, size=target_size, mode='bilinear', align_corners=False)

        # 특성 연결
        out = torch.cat([out_f1, out_f2, out_f3, out_f4], dim=1)

        # 최종 컨볼루션 및 업샘플링 적용
        out = self.final_convolution(out)
        out = self.final_upsample(out)
        return out





class FireSegmentationModel(nn.Module):
    """The Fire Segmentation Model.

    Attributes
    ----------
    encoder : Encoder
        The encoder module
    decoder : Decoder
        The encoder module
    """
    def __init__(self, input_size: Tuple[int, int], device: str) -> None:
        """Initialize the Fire Segmentation Model.

        Parameters
        ----------
        input_size : (int, int)
            The size of the input image.
        device : str
            The device where the model will be loaded.
        """
        super().__init__()
        # Set the encoder module.
        self.encoder = Encoder()
        # Set the decoder module.
        self.decoder = Decoder(target_size=input_size)
        # Assign the model to the desired device.
        self.to(device)

    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:
        """Forward pass of the Fire Segmentation Model.

        Parameters
        ----------
        x : FloatTensor
            The input tensor.

        Returns
        -------
        FloatTensor
            The output tensor containing the final segmentation mask of
            the background and the foreground.
        """
        # Apply the encoder block to the input and get the
        # intermediate features.
        f1, f2, f3, f4 = self.encoder(x)
        # Decode the final segmentation mask.
        out = self.decoder(f1, f2, f3, f4)
        return out









device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FireSegmentationModel(IMAGE_SIZE, device)
# 옵티마이저 및 손실 함수 설정
optimizer = torch.optim.Adam(model.parameters())
criterion = nn.BCEWithLogitsLoss()

# def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=1, print_every=100, output_dir='./train_output'):
#     best_model_wts = copy.deepcopy(model.state_dict())
#     best_loss = float('inf')
    
#     for epoch in range(num_epochs):
#         print('Epoch {}/{}'.format(epoch, num_epochs - 1))
#         print('-' * 10)
        
#         for phase in ['train', 'val']:
#             if phase == 'train':
#                 model.train()
#                 data_loader = train_loader
#             else:
#                 model.eval()
#                 data_loader = val_loader
            
#             running_loss = 0.0
            
#             for batch_idx, (inputs, masks) in enumerate(data_loader):
#                 inputs = inputs.to(device)
#                 masks = masks.to(device)

#                 optimizer.zero_grad()

#                 with torch.set_grad_enabled(phase == 'train'):
#                     outputs = model(inputs)
#                     loss = criterion(outputs, masks)

#                     if phase == 'train':
#                         loss.backward()
#                         optimizer.step()

#                 running_loss += loss.item() * inputs.size(0)
                
#                 if phase == 'train' and (batch_idx + 1) % print_every == 0:
#                     print(f"Epoch {epoch} Batch {batch_idx+1}/{len(data_loader)} Loss: {loss.item():.4f}")

#             epoch_loss = running_loss / len(data_loader.dataset)
#             print('{} Loss: {:.4f}'.format(phase, epoch_loss))

#             # 여기서 가중치 저장 조건을 확인하고 저장합니다.
#             if epoch == 0 or epoch == num_epochs-1 or (epoch + 1) % 3 == 0:
#                 torch.save(model.state_dict(), os.path.join(output_dir, f"model_epoch_{epoch}.pth"))
#                 print(f"Saved model at epoch {epoch}")

#             if phase == 'val' and epoch_loss < best_loss:
#                 best_loss = epoch_loss
#                 best_model_wts = copy.deepcopy(model.state_dict())
        
#         print()

#     print('Best val Loss: {:4f}'.format(best_loss))

#     # 최적의 모델 가중치를 로드합니다.
#     model.load_state_dict(best_model_wts)
#     return model

# # 모델 학습 및 검증
# best_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=EPOCHS)

# 필요시 모델 가중치 로드
# model.load_state_dict(torch.load(os.path.join(OUTPUT_DIR, FINAL_WEIGHTS_OUTPUT)))
model.load_state_dict(torch.load(os.path.join('C:/Users/PC/FireSegmentation/train_output/model_epoch_0.pth')))
model.to(device)

import joblib

y_pred_dict = {}

model.eval()  # Ensure the model is in eval mode

with torch.no_grad():
    for i in test_meta['test_img']:
        img_path = os.path.join('./dataset/test_img/', i)
        img = get_img_762bands_test(img_path)
        img = torch.tensor(img).unsqueeze(0).to(device)  # Add batch dimension

        # Workaround: Repeat the input to increase "batch" size
        img_repeated = img.repeat(2, 1, 1, 1)  # Repeat the input to simulate a batch

        outputs = model(img_repeated)
        outputs = outputs[0:1]  # Take the output corresponding to the original input

        preds = torch.sigmoid(outputs).data > 0.5
        preds = preds.cpu().numpy().astype(np.uint8)

        y_pred_dict[i] = preds[0, 0, :, :]

# Save the predictions
joblib.dump(y_pred_dict, './y_pred.pkl')
